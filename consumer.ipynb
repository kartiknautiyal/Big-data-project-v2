{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc4d53f-2b13-43a1-a554-82b06b0cf764",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaedfdf-fc0d-4ff0-b21d-a9097fb241fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"global\": {\n",
      "    \"kafka_bootstrap_servers\": \"kafka:9092\",\n",
      "    \"kafka_topic\": \"test-structured-streaming\",\n",
      "    \"kafka_consumer_group\": \"ss_job\",\n",
      "    \"max_records_per_batch\": 20\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"consumer\")\n",
    "\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1835df41-fbd2-4e46-a234-912f80d507c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{\"2.12\"}:{\"3.3.0\"}',\n",
    "    'org.apache.kafka:kafka-clients:2.8.0',\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.0\",\n",
    "    \"com.google.guava:guava:21.0\",\n",
    "    \"org.apache.httpcomponents:httpcore:4.4.8\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92965dc3-a7c7-40ea-bfdb-1e02340b3713",
   "metadata": {},
   "source": [
    "# 2. Initialize Spark with Kafa Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed753070-30f8-4883-97aa-4147a6532147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.google.guava#guava added as a dependency\n",
      "org.apache.httpcomponents#httpcore added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6f782454-06ad-4326-9f34-8e1c0d277c00;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.google.guava#guava;21.0 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.8 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0!spark-sql-kafka-0-10_2.12.jar (115ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.0/hadoop-aws-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.0!hadoop-aws.jar (122ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/guava/21.0/guava-21.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#guava;21.0!guava.jar(bundle) (253ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.8/httpcore-4.4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.8!httpcore.jar (75ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0!spark-token-provider-kafka-0-10_2.12.jar (57ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (391ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (53ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (63ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (51ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (2183ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (101ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (195ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (67ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (1412ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (55ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.563/aws-java-sdk-bundle-1.11.563.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.563!aws-java-sdk-bundle.jar (9168ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (91ms)\n",
      ":: resolution report :: resolve 8082ms :: artifacts dl 14465ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;21.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.8 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 by [org.apache.kafka#kafka-clients;2.8.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   17  |   17  |   1   ||   17  |   17  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6f782454-06ad-4326-9f34-8e1c0d277c00\n",
      "\tconfs: [default]\n",
      "\t17 artifacts copied, 0 already retrieved (187461kB/482ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/16 07:22:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:consumer:Spark Driver memory: None\n",
      "INFO:consumer:Spark Executor memory: None\n",
      "INFO:consumer:Loaded jars:\n",
      "[\n",
      "  \"spark://b4d43d382907:46871/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.lz4_lz4-java-1.8.0.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/com.amazonaws_aws-java-sdk-bundle-1.11.563.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/commons-logging_commons-logging-1.1.3.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/com.google.code.findbugs_jsr305-3.0.0.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.apache.kafka_kafka-clients-2.8.1.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.spark-project.spark_unused-1.0.0.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.apache.hadoop_hadoop-aws-3.3.0.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/com.google.guava_guava-21.0.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.apache.httpcomponents_httpcore-4.4.8.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.apache.commons_commons-pool2-2.11.1.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.slf4j_slf4j-api-1.7.32.jar\",\n",
      "  \"spark://b4d43d382907:46871/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Configuration and set application name\n",
    "conf = SparkConf().setAppName(\"KafkaExp\")\n",
    "\n",
    "# Default pyspark installation lacks kafka consumer libraries. Install kafka-client libs manually\n",
    "kafka_packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{\"2.12\"}:{\"3.3.0\"}',\n",
    "    'org.apache.kafka:kafka-clients:2.8.0',\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.0\",\n",
    "    \"com.google.guava:guava:21.0\",\n",
    "    \"org.apache.httpcomponents:httpcore:4.4.8\"\n",
    "]\n",
    "\n",
    "# Provide kafka jar paths to driver and executors\n",
    "kafka_jar_paths = '/mnt/home/prathyush/.ivy2/jars/'.join([\n",
    "    \"org.apache.hadoop_hadoop-client-runtime-3.3.2.jar\",\n",
    "    \"org.apache.kafka_kafka-clients-2.8.1.jar\",\n",
    "    \"hadoop-aws-2.7.5.jar\",\n",
    "    \"aws-java-sdk-core-1.12.268.jar\"\n",
    "])\n",
    "\n",
    "# Connect to Spark cluster (Cluster mode instead of local mode)\n",
    "conf = (conf.setMaster('spark://spark:7077')\n",
    "        .set('spark.jars.packages', ','.join(kafka_packages))\n",
    "        .set('spark.driver.extraClassPath', '/mnt/home/prathyush/.ivy2/jars/*')\n",
    "        .set('spark.executor.extraClassPath', '/mnt/home/prathyush/.ivy2/jars/*')\n",
    "        )\n",
    "\n",
    "# Create spark context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "logger.info(f\"Spark Driver memory: {sc._conf.get('spark.driver.memory')}\")\n",
    "logger.info(f\"Spark Executor memory: {sc._conf.get('spark.executor.memory')}\")\n",
    "logger.info(\n",
    "    f'Loaded jars:\\n{json.dumps((sc._jsc.sc().listJars().toList().toString().replace(\"List(\", \"\").replace(\")\", \"\").split(\", \")), indent=2)}')\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession(sc)\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "spark.conf.set(\"parquet.enable.summary-metadata\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b4715-66b6-4a47-8511-fd57bf5a0b52",
   "metadata": {},
   "source": [
    "# 3. Test Kafka topic and connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd64ee87-38ef-48fc-9c3a-cb02eea054b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka Connection successful!\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "def test_kafka_connection(broker_conf:dict) -> None:\n",
    "    \"\"\"\n",
    "    Function to test kafka connection\n",
    "    :param broker_conf: Broker configuration\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "    client = AdminClient(broker_conf)\n",
    "    topics = client.list_topics().topics\n",
    "    if not topics:\n",
    "        raise RuntimeError()\n",
    "    print(\"Kafka Connection successful!\")\n",
    "\n",
    "\n",
    "broker_conf = {\n",
    "    'bootstrap.servers': config[\"global\"][\"kafka_bootstrap_servers\"]\n",
    "}\n",
    "\n",
    "# Test kafka connection\n",
    "test_kafka_connection(broker_conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3505e-9cef-4148-a0af-10643d9d94c7",
   "metadata": {},
   "source": [
    "# 5. Configure Spark-Kafka consumer options and Subscribe to Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5fe9d1e-73f6-42e1-bcf7-0cc9a05d8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType, ArrayType,FloatType,DoubleType\n",
    "schema = StructType().add(\"VendorID\", StringType(), True)\\\n",
    "                    .add(\"total_amount\",FloatType(),True)\\\n",
    "                    .add(\"trip_distancet\",DoubleType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9c1b40-46a6-4629-834e-5df667667891",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Configure spark kafka client options\n",
    "spark_kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": config[\"global\"][\"kafka_bootstrap_servers\"],\n",
    "    \"subscribe\": config[\"global\"][\"kafka_topic\"],\n",
    "    \"kafka.group.id\": config[\"global\"][\"kafka_consumer_group\"],\n",
    "    \"maxOffsetsPerTrigger\": config[\"global\"][\"max_records_per_batch\"],\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "}\n",
    "\n",
    "# Enable spark read stream\n",
    "df = spark.readStream.format(\"kafka\").options(**spark_kafka_options).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6ea91-8bee-4e9a-aecb-1d4dc4d1d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"mongo\").mode(\"append\").option(\"uri\",mongoURL).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b022a19-12ed-46ab-800f-97503e2a6917",
   "metadata": {},
   "source": [
    "# 6. Start spark structred streaming job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "504b9fac-d6d0-41f9-aad5-65edd3548abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(awards_year):\n",
    "    temp = []\n",
    "    if awards_year is None:\n",
    "        return []\n",
    "    for year in awards_year:\n",
    "        temp.append(int(year))\n",
    "    return [min(temp),max(temp)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3628d30d-1723-43b1-976b-6f25520ebfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, John, Backus}| 83|           4|[1967, 1993]|\n",
      "|{null, John, McCa...| 84|           3|[1971, 1990]|\n",
      "|{null, Grace, Hop...| 86|           4|[1969, 1991]|\n",
      "|{null, Kristen, N...| 76|           2|[1999, 2001]|\n",
      "|{null, Ole-Johan,...| 71|           2|[1999, 2001]|\n",
      "|{null, Guido, van...| 66|           1|[2001, 2003]|\n",
      "|{null, Dennis, Ri...| 70|           2|[1983, 2011]|\n",
      "|{Matz, Yukihiro, ...| 57|           1|[2011, 2011]|\n",
      "|{null, James, Gos...| 67|           1|[2002, 2007]|\n",
      "|{null, Martin, Od...| 67|           1|          []|\n",
      "|{null, John, Backus}| 83|           4|[1967, 1993]|\n",
      "|{null, John, McCa...| 84|           3|[1971, 1990]|\n",
      "|{null, Grace, Hop...| 86|           4|[1969, 1991]|\n",
      "|{null, Kristen, N...| 76|           2|[1999, 2001]|\n",
      "|{null, Ole-Johan,...| 71|           2|[1999, 2001]|\n",
      "|{null, Guido, van...| 66|           1|[2001, 2003]|\n",
      "|{null, Dennis, Ri...| 70|           2|[1983, 2011]|\n",
      "|{Matz, Yukihiro, ...| 57|           1|[2011, 2011]|\n",
      "|{null, James, Gos...| 67|           1|[2002, 2007]|\n",
      "|{null, Martin, Od...| 67|           1|          []|\n",
      "+--------------------+---+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 20\n",
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, John, Backus}| 83|           4|[1967, 1993]|\n",
      "|{null, John, McCa...| 84|           3|[1971, 1990]|\n",
      "|{null, Grace, Hop...| 86|           4|[1969, 1991]|\n",
      "|{null, Kristen, N...| 76|           2|[1999, 2001]|\n",
      "|{null, Ole-Johan,...| 71|           2|[1999, 2001]|\n",
      "|{null, Guido, van...| 66|           1|[2001, 2003]|\n",
      "|{null, Dennis, Ri...| 70|           2|[1983, 2011]|\n",
      "|{Matz, Yukihiro, ...| 57|           1|[2011, 2011]|\n",
      "|{null, James, Gos...| 67|           1|[2002, 2007]|\n",
      "|{null, Martin, Od...| 67|           1|          []|\n",
      "|{null, John, Backus}| 83|           4|[1967, 1993]|\n",
      "|{null, John, McCa...| 84|           3|[1971, 1990]|\n",
      "|{null, Grace, Hop...| 86|           4|[1969, 1991]|\n",
      "|{null, Kristen, N...| 76|           2|[1999, 2001]|\n",
      "|{null, Ole-Johan,...| 71|           2|[1999, 2001]|\n",
      "|{null, Guido, van...| 66|           1|[2001, 2003]|\n",
      "|{null, Dennis, Ri...| 70|           2|[1983, 2011]|\n",
      "|{Matz, Yukihiro, ...| 57|           1|[2011, 2011]|\n",
      "|{null, James, Gos...| 67|           1|[2002, 2007]|\n",
      "|{null, Martin, Od...| 67|           1|          []|\n",
      "+--------------------+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lambda Function for processing each batch of record\n",
    "def process_batch(batch_df, batch_idx):\n",
    "    print(f\"{batch_idx} | {batch_df.count()}\")\n",
    "    batch_df = batch_df.selectExpr(\"CAST(value AS STRING)\").select(from_json(\"value\", schema).alias(\"data\"))\n",
    "    batch_df = batch_df.selectExpr(\"data.name\",\"data.contribs\",\"data.awards\",\"CAST(data.birth AS DATE) as birth\",\"CAST(data.death AS DATE) as death\")\n",
    "    batch_df = batch_df.withColumn('death', F.when(F.col('death').isNull(), datetime.datetime.now().date()).otherwise(F.col('death')))\n",
    "    batch_df= batch_df.withColumn(\"age\", F.year(F.col(\"death\"))-F.year(F.col(\"birth\")))\n",
    "    convertUDF = F.udf(lambda z: min_max(z))\n",
    "\n",
    "    batch_df = batch_df.select(F.col(\"name\"), F.col(\"age\"),F.size(F.col(\"contribs\")).alias(\"num_contribs\"), convertUDF(F.col(\"awards.year\")).alias(\"min_max\"))\n",
    " \n",
    "    # batch_df.write.mode('append').parquet(\"parqi.parquet\")\n",
    "    batch_df.write.mode('append').format(\"console\")\n",
    "    batch_df.collect()\n",
    "    batch_df.show()\n",
    "    return batch_df\n",
    "\n",
    "# Structred streaming query\n",
    "\n",
    "query = df.writeStream.trigger(processingTime='10 seconds').foreachBatch(process_batch).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08164cc-18ff-4e14-82b2-6b69af325de0",
   "metadata": {},
   "source": [
    "# 7. Monitor structred streaming job progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34f9d7b-bd97-42db-b5c9-74373f999bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 | 19\n",
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, John, Backus}| 83|           4|[1967, 1993]|\n",
      "|{null, John, McCa...| 84|           3|[1971, 1990]|\n",
      "|{null, Grace, Hop...| 86|           4|[1969, 1991]|\n",
      "|{null, Kristen, N...| 76|           2|[1999, 2001]|\n",
      "|{null, Ole-Johan,...| 71|           2|[1999, 2001]|\n",
      "|{null, Guido, van...| 66|           1|[2001, 2003]|\n",
      "|{null, Dennis, Ri...| 70|           2|[1983, 2011]|\n",
      "|{Matz, Yukihiro, ...| 57|           1|[2011, 2011]|\n",
      "|{null, James, Gos...| 67|           1|[2002, 2007]|\n",
      "|{null, Martin, Od...| 67|           1|          []|\n",
      "|{null, John, Backus}| 83|           4|[1967, 1993]|\n",
      "|{null, John, McCa...| 84|           3|[1971, 1990]|\n",
      "|{null, Grace, Hop...| 86|           4|[1969, 1991]|\n",
      "|{null, Ole-Johan,...| 71|           2|[1999, 2001]|\n",
      "|{null, Guido, van...| 66|           1|[2001, 2003]|\n",
      "|{null, Dennis, Ri...| 70|           2|[1983, 2011]|\n",
      "|{Matz, Yukihiro, ...| 57|           1|[2011, 2011]|\n",
      "|{null, James, Gos...| 67|           1|[2002, 2007]|\n",
      "|{null, Martin, Od...| 67|           1|          []|\n",
      "+--------------------+---+------------+------------+\n",
      "\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 | 2\n",
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, John, Backus}| 83|           4|[1967, 1993]|\n",
      "|{null, John, McCa...| 84|           3|[1971, 1990]|\n",
      "+--------------------+---+------------+------------+\n",
      "\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 | 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, Grace, Hop...| 86|           4|[1969, 1991]|\n",
      "|{null, Kristen, N...| 76|           2|[1999, 2001]|\n",
      "+--------------------+---+------------+------------+\n",
      "\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 | 3\n",
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, Ole-Johan,...| 71|           2|[1999, 2001]|\n",
      "|{null, Guido, van...| 66|           1|[2001, 2003]|\n",
      "|{null, Dennis, Ri...| 70|           2|[1983, 2011]|\n",
      "+--------------------+---+------------+------------+\n",
      "\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 | 2\n",
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{Matz, Yukihiro, ...| 57|           1|[2011, 2011]|\n",
      "|{null, James, Gos...| 67|           1|[2002, 2007]|\n",
      "+--------------------+---+------------+------------+\n",
      "\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 | 2\n",
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, Martin, Od...| 67|           1|          []|\n",
      "|{null, John, Backus}| 83|           4|[1967, 1993]|\n",
      "+--------------------+---+------------+------------+\n",
      "\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 | 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, John, McCa...| 84|           3|[1971, 1990]|\n",
      "|{null, Grace, Hop...| 86|           4|[1969, 1991]|\n",
      "|{null, Kristen, N...| 76|           2|[1999, 2001]|\n",
      "+--------------------+---+------------+------------+\n",
      "\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 | 1\n",
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, Ole-Johan,...| 71|           2|[1999, 2001]|\n",
      "+--------------------+---+------------+------------+\n",
      "\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n",
      "{'message': 'Waiting for next trigger', 'isDataAvailable': True, 'isTriggerActive': False}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m query\u001b[38;5;241m.\u001b[39mstatus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misDataAvailable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m query\u001b[38;5;241m.\u001b[39mstatus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misTriggerActive\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(query\u001b[38;5;241m.\u001b[39mstatus)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Stop query\u001b[39;00m\n\u001b[1;32m     11\u001b[0m query\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 | 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, Guido, van...| 66|           1|[2001, 2003]|\n",
      "|{null, Dennis, Ri...| 70|           2|[1983, 2011]|\n",
      "|{Matz, Yukihiro, ...| 57|           1|[2011, 2011]|\n",
      "+--------------------+---+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 | 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, James, Gos...| 67|           1|[2002, 2007]|\n",
      "+--------------------+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add startup delay\n",
    "time.sleep(5)\n",
    "# Update Job Status\n",
    "\n",
    "print(query.status)\n",
    "while query.status['isDataAvailable'] or query.status['isTriggerActive']:\n",
    "    print(query.status)\n",
    "    time.sleep(5)\n",
    "\n",
    "# Stop query\n",
    "query.stop()\n",
    "\n",
    "logger.info(\"Structred streaming job completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4871dfb9-1de8-4ed7-9280-fc93f03083bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+------------+\n",
      "|                name|age|num_contribs|     min_max|\n",
      "+--------------------+---+------------+------------+\n",
      "|{null, John, Backus}| 83|           4|[1967, 1993]|\n",
      "|{null, John, McCa...| 84|           3|[1971, 1990]|\n",
      "|{null, Grace, Hop...| 86|           4|[1969, 1991]|\n",
      "|{null, Kristen, N...| 76|           2|[1999, 2001]|\n",
      "|{null, Ole-Johan,...| 71|           2|[1999, 2001]|\n",
      "|{null, Guido, van...| 66|           1|[2001, 2003]|\n",
      "|{null, Dennis, Ri...| 70|           2|[1983, 2011]|\n",
      "|{Matz, Yukihiro, ...| 57|           1|[2011, 2011]|\n",
      "|{null, James, Gos...| 67|           1|[2002, 2007]|\n",
      "|{null, Martin, Od...| 67|           1|          []|\n",
      "+--------------------+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"parqi.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd301d9-4931-473c-bcb5-1153d9cb5b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "5eab687acb3ddfe264791fe74937bc8765d50ea3df4d9a9a62730aa97325aae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
