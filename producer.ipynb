{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0d2fc1-e715-48e6-8233-b428202f0702",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd042037-da4c-49cf-9046-e62df7838e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"global\": {\n",
      "    \"kafka_bootstrap_servers\": \"kafka:9092\",\n",
      "    \"kafka_topic\": \"test-structured-streaming\",\n",
      "    \"kafka_consumer_group\": \"ss_job\",\n",
      "    \"max_records_per_batch\": 20\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "from confluent_kafka import Producer\n",
    "import typing\n",
    "import logging\n",
    "import csv\n",
    "import random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"producer\")\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bad7d-9667-4d98-a5ed-1e24c1f62af4",
   "metadata": {},
   "source": [
    "# Amazon s3\n",
    "\n",
    "Authenticate and access the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6addd095-e9ad-4f96-a1f2-691dc27ae5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucket.objects.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f706cd1-72d1-47e9-be3f-ef1f43f71f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3',\n",
    "         aws_access_key_id=\"AKIAWZKOGZHEBXUV4MGL\",\n",
    "         aws_secret_access_key= \"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\")\n",
    "\n",
    "bucket = s3.Bucket(\"tempsource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3954e68-2ef6-455c-8e41-399da2e5728f",
   "metadata": {},
   "source": [
    "Creates an iterable object which can be decoded by csv codecs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddb6901-40a0-4d49-b3e4-dc233f840d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "\n",
    "for obj in bucket.objects.all():\n",
    "    it = obj.get()[\"Body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "012ba20d-21a8-476c-8d8f-62f42d06b4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<botocore.response.StreamingBody object at 0x7f06ddb6a070>\n"
     ]
    }
   ],
   "source": [
    "print(iterable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce64275-ba00-4cb7-9b39-399f047e5b9d",
   "metadata": {},
   "source": [
    "Lazy evaluation. (dont want to load everything at once hence record by record.)\n",
    "output type : dictionary (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fbd55f-e1d3-45c6-be20-8b4b6009b207",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for record in csv.DictReader(codecs.getreader(\"utf-8\")(it)):\n",
    "    print(record)\n",
    "    print(type(record))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5610f-347c-4d10-a250-2a91cb04a0d5",
   "metadata": {},
   "source": [
    "# 2. Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae8f82c-cacf-47e7-820a-98fc8a445cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_generator() -> str:\n",
    "    with open(\"taxi.csv\",encoding = 'utf-8-sig') as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            yield(row)\n",
    "data = data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c14863da-7b0c-4625-8337-a7a49431fc60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'it' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m(record)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     f = open('data.json')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     json_data = json.load(f)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     for record in json_data:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Push data to kafka\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m data \u001b[38;5;241m=\u001b[39m data_generator(\u001b[43mit\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'it' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def data_generator(iterable_s3_object) -> str:\n",
    "    \n",
    "    \n",
    "    for record in csv.DictReader(codecs.getreader(\"utf-8\")(iterable_s3_object)):\n",
    "        yield(record)\n",
    "    \n",
    "#     f = open('data.json')\n",
    "#     json_data = json.load(f)\n",
    "#     for record in json_data:\n",
    "#         yield(record)\n",
    "    \n",
    "\n",
    "\n",
    "# Push data to kafka\n",
    "data = data_generator(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa127271-03a7-45d1-83f2-c8d07ea68c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4090daf3-306b-4162-a999-6fcf1560ba49",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object data_generator at 0x7f0a45fcff20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def data_generator() -> str:\n",
    "    \n",
    "    f = open('data.json')\n",
    "    json_data = json.load(f)\n",
    "    for record in json_data:\n",
    "        yield(record)\n",
    "    \n",
    "\n",
    "\n",
    "# Push data to kafka\n",
    "data = data_generator()\n",
    "data_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e7021-9e97-4eda-9a2f-234372563da9",
   "metadata": {},
   "source": [
    "# 3. Build Kafka Producer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f80b66-2d79-427f-9056-3ec533103b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "broker_conf = {\n",
    "    'bootstrap.servers': config[\"global\"][\"kafka_bootstrap_servers\"],\n",
    "}\n",
    "\n",
    "producer_conf = {**broker_conf}\n",
    "\n",
    "class KafkaProducer(object):\n",
    "    def __init__(self, broker_conf, debug=False):\n",
    "        self.broker_conf = broker_conf\n",
    "        self.producer = Producer(self.broker_conf)\n",
    "        self.debug = debug\n",
    "\n",
    "    def delivery_report(self, err, msg):\n",
    "        \"\"\" Called once for each message produced to indicate delivery result.\n",
    "            Triggered by poll() or flush(). \"\"\"\n",
    "        if err is not None:\n",
    "            logger.error('Message delivery failed: {}'.format(err))\n",
    "        else:\n",
    "            # if self.debug:\n",
    "            logger.info('Message delivered to topic: {} [parition={}]'.format(msg.topic(), msg.partition()))\n",
    "\n",
    "    def push(self, data: typing.List[str], topic: str, delay_in_secs: int):\n",
    "        start_time = time.time()\n",
    "        num_records = 0\n",
    "        \n",
    "        for record in data:\n",
    "            self.producer.poll(0)\n",
    "            \n",
    "            time.sleep(random.randint(1,10))\n",
    "            self.producer.produce(topic,str(record),callback = self.delivery_report)\n",
    "            num_records+=1\n",
    "\n",
    "        ...\n",
    "        self.producer.flush()\n",
    "\n",
    "        end_time = time.time()\n",
    "        completion_time = end_time-start_time\n",
    "        logger.info(\n",
    "            f\"Pushed {num_records} records with {delay_in_secs} secs delay. Task completed in {completion_time:.2f} secs\")\n",
    "        return num_records, completion_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37516e-48f5-45a0-b5ee-0a4701100076",
   "metadata": {},
   "source": [
    "# 4. Push data to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1d305dc-beeb-4b4d-91f4-b0ae54655602",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n",
      "INFO:producer:Message delivered to topic: test-structured-streaming [parition=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m kafka_produce \u001b[38;5;241m=\u001b[39m KafkaProducer(broker_conf\u001b[38;5;241m=\u001b[39mbroker_conf, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m num_records, _ \u001b[38;5;241m=\u001b[39m \u001b[43mkafka_produce\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtopic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglobal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka_topic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay_in_secs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mKafkaProducer.push\u001b[0;34m(self, data, topic, delay_in_secs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproducer\u001b[38;5;241m.\u001b[39mpoll(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproducer\u001b[38;5;241m.\u001b[39mproduce(topic,\u001b[38;5;28mstr\u001b[39m(record),callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelivery_report)\n\u001b[1;32m     31\u001b[0m     num_records\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kafka_produce = KafkaProducer(broker_conf=broker_conf, debug=True)\n",
    "num_records, _ = kafka_produce.push(data=data,  topic=config[\"global\"][\"kafka_topic\"], delay_in_secs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "bdff9bcfa0ccc71486e4dde6ff189013f13e2d488bfbfec38844d4916b87ef74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
